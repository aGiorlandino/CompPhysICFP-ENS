{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q_3M2R8elab"
      },
      "source": [
        "# Markov matrices\n",
        "\n",
        "The goal of this notebook is to investigate the properties of Markov matrices. The Markov matrix\n",
        "(or transition matrix) has entries $p_{a \\to b}$ corresponding to the probability to have\n",
        "a transition from $a$ to $b$. The entries are organized as follows (we take $a \\in [1,9]$ for\n",
        "this illustration):\n",
        "\n",
        "$$\n",
        "T=\n",
        "\\begin{pmatrix}\n",
        "p_{1 \\to 1} & p_{2\\to 1} & \\cdots & p_{9 \\to 1} \\\\\n",
        "p_{1\\to 2} & p_{2\\to 2} & \\cdots & p_{9 \\to 2} \\\\\n",
        "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "p_{1\\to 9} & p_{2\\to 9} & \\cdots & p_{9 \\to 9} \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "The elements of the Markov matrix $T$ must satisfy two conditions:\n",
        "\n",
        "* They are non negative (because they are probabilities)\n",
        "* The elements of each column sum to one. For example $$\\sum_{i=1}^9 p_{1 \\to i} =1$$ as the probability of doing something, being on the position 1, is clearly one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97yYC53CEZH0"
      },
      "source": [
        "## Evolution and probability conservation\n",
        "\n",
        "The Markov matrix describes how the probability distribution evolves in time. Suppose the different configurations $a$ are distributed according to probabilities $\\pi_a^t$ at time step $t$. This can be\n",
        "expressed by a vector\n",
        "\n",
        "$$\n",
        "\\pi^t = \\begin{pmatrix} \\pi_1^t \\\\ \\pi_2^t \\\\ \\vdots \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "At the next time step, the probability distribution will be\n",
        "\n",
        "$$\n",
        "\\pi^{t+1} = T \\pi^t\n",
        "$$\n",
        "\n",
        "Clearly, if the initial probability distribution is $\\pi^0$, then the probability\n",
        "distribution at time step $t$ is $\\pi^t = T^t \\pi^0$. It is simple to show that the normalization of the probability distribution is conserved:\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^9  \\pi_j^{t+1} = \\sum_{j,i=1}^9   T_{ji} \\pi_i^{t} = \\sum_{i=1}^9 \\left(\\sum_{j=1}^9  T_{ji} \\right) \\pi_i^t =\\sum_{i=1}^9 \\pi_i^t=1\n",
        "$$\n",
        "\n",
        "Note that having $\\sum_{j=1}^9 T_{ji} <1$ would imply a probability loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG2qD57UFYwB"
      },
      "source": [
        "## Markov matrix spectrum\n",
        "Markov matrices are in general non symmetric (e.g. the matric T for the inhomogeneus pebble game). As a consequence: \n",
        "* the eigenvalues can be complex \n",
        "\n",
        "* the eigenvectors of $T$ and $T^T$ may be different (eigenvalues are instead  the same as $\\text{det}[T-\\lambda I]= \\text{det}[T^T-\\lambda I]$), \n",
        "\n",
        "Among the eigenvalues there is for sure the eigenvalue $\\lambda =1$:\n",
        "$$\n",
        "T^T \\begin{pmatrix}\n",
        "1  \\\\\n",
        "1 \\\\\n",
        "\\vdots   \\\\\n",
        "1  \n",
        "\\end{pmatrix}\n",
        "= \\begin{pmatrix}\n",
        "1  \\\\\n",
        "1 \\\\\n",
        "\\vdots   \\\\\n",
        "1  \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Note that the elements of the eigenvector $\\begin{pmatrix}\n",
        "1  \\\\\n",
        "1 \\\\\n",
        "\\vdots   \\\\\n",
        "1  \n",
        "\\end{pmatrix}$ are all positve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1ShFvEFsmP"
      },
      "source": [
        "### Perron Froebenius Theorem (simpler version)\n",
        "\n",
        "Let us first discuss the case where the elements of a Markov matrix are all positive (this makes statements simpler). Then the theorem shows that:\n",
        "\n",
        "1.   The eigenvalue with maximal modulus is positive and non degenerate.\n",
        "2.   the associated eigenvector is the only one with all entries positive.\n",
        "\n",
        "There are many consequences:\n",
        "\n",
        "* $\\lambda_1=1$ is then the maximal eigenvalue of $T^T$. It is then maximal for $T$ as well. Moreover, its associated eigenvector, $\\pi_1$ is the stationary probability of the Markov chain:\n",
        "\n",
        "$$T \\pi_1 =\\pi_1 $$\n",
        "\n",
        "* All the other eigenvalues have a smaller modulus. In particular, the second largest eignevalue $\\lambda_2 =|\\lambda_2| e^{i \\theta_2}$ plays an important role. Indeed, an arbitrary initial condition $\\pi^{t=0}$ can be decomposed on the basis of the eigenvectors of $T$:\n",
        "\n",
        "$$\\pi^{t=0}= \\pi_1+ c_2 \\pi_2 +c_3 \\pi_3+\\ldots$$\n",
        "\n",
        "At the first step of the Markov chain we have\n",
        "\n",
        "$$\\pi^{t=1}= \\pi_1+ \\lambda_2 c_2 \\pi_2 + \\lambda_3 c_3 \\pi_3+\\ldots$$\n",
        "\n",
        "At the second step of the Markov chain we have\n",
        "\n",
        "$$\\pi^{t=2}= \\pi_1+ \\lambda_2^2 c_2 \\pi_2 + \\lambda_3^2 c_3 \\pi_3+\\ldots$$\n",
        "\n",
        "After $t$ steps\n",
        "\n",
        "$$\\pi^{t}= \\pi_1+ \\lambda_2^t c_2 \\pi_2 + \\lambda_3^t c_3 \\pi_3+\\ldots  \\\\ = \\pi_1 + |\\lambda_2|^t e^{i \\theta_2 t} c_2 \\pi_2 +\\ldots$$\n",
        "\n",
        "Note that because $|\\lambda_2|<1$ the Markov chain converges to the stationary state exponentially fast. If\n",
        "we write $|\\lambda_2|^t = \\exp(-t/ \\tau)$, we find\n",
        "\n",
        "$$\\tau = -\\frac{1}{\\ln |\\lambda_2|}$$\n",
        "\n",
        "A Markov chain Monte Carlo is then able to produce an unbiased configuaration after $\\tau$ steps. The closer $|\\lambda_2|$ is to 1, the slower is the convergence of the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vanOvD_nEZqC"
      },
      "source": [
        "### Special cases\n",
        "\n",
        "If some of the elements of $T$ are zero, the eigenvalue $\\lambda_1=1$ is still maximal but it may be degenerate. The associated eigenvector is still a stationary state, but the convergence to that state may be problematic.\n",
        "\n",
        "#### *Periodic Markov chains*\n",
        "\n",
        "A Markov chain is **periodic** when $\\lambda_2 \\ne 1$ but $|\\lambda_2| =1$. Consider the following examples. Provide a graphical presentation of the Markov chain. Which one is periodic? \n",
        "$$\n",
        "T_1=\n",
        "\\begin{pmatrix}\n",
        "0 & 1/2&  0 \\\\\n",
        "1 & 0& 1  \\\\\n",
        "0 & 1/2 & 0 \n",
        "\\end{pmatrix} \\quad \n",
        "T_2=\n",
        "\\begin{pmatrix}\n",
        "0 & 0&  1 \\\\\n",
        "1 & 0& 0  \\\\\n",
        "0 & 1 & 0 \n",
        "\\end{pmatrix}\n",
        " \\quad \n",
        "T_3=\n",
        "\\begin{pmatrix}\n",
        "0 & 1/2&  1 \\\\\n",
        "1 & 0& 0 \\\\\n",
        "0 & 1/2 & 0 \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "In order to avoid periodicity it is enough to have  slightly positive\n",
        "diagonal elements.\n",
        "\n",
        "#### *Reducible Markov chains*\n",
        "\n",
        "A Markov chain is **reducible** when $\\lambda_1 =1$ is degenerate.  Consider the following examples. Provide a graphical presentation of the Markov chain. Which one is reducible? \n",
        "\n",
        "$$\n",
        "T_1=\n",
        "\\begin{pmatrix}\n",
        "0 & 1&  0 \\\\\n",
        "1 & 0& 0  \\\\\n",
        "0 & 0 & 1 \n",
        "\\end{pmatrix} \\quad \n",
        "T_2=\n",
        "\\begin{pmatrix}\n",
        "0 & 1&  1/2 \\\\\n",
        "1 & 0& 1/2  \\\\\n",
        "0 & 0 & 0 \n",
        "\\end{pmatrix}\n",
        " \\quad \n",
        "T_3=\n",
        "\\begin{pmatrix}\n",
        "0 & 1/2&  1 \\\\\n",
        "1 & 0& 0 \\\\\n",
        "0 & 1/2 & 0 \n",
        "\\end{pmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOWqsy9TJKn_"
      },
      "source": [
        "In the following, you will consider simple problems and check whether the probability distribution\n",
        "eventually becomes stationary, how quickly this happens and under what conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPgaFc1nnhC7"
      },
      "source": [
        "## Markov matrix for the uniform distribution\n",
        "\n",
        "We will start with the simple example of a uniform distribution\n",
        "probability over a $3 \\times 3$ lattice. The goal is to\n",
        "construct the Markov matrix in such a way that all cells will be visited with\n",
        "the same probability.\n",
        "During the lecture, we have seen that this distribution can be obtained by\n",
        "choosing transition probabilities that are $1/4$ for every neighbor\n",
        "of a cell. If the transition goes out of the lattice, one remains on\n",
        "the cell. In your codes, you can use the following convention to label the\n",
        "cells in the lattice:\n",
        "\n",
        "&nbsp;\n",
        "<center>\n",
        "<img src=\"https://gist.github.com/mferrero/ae328ab0e3a0d3d7181a007daf5a373a/raw/cells.png\" width=150 height=150 />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9OMazSaG1Hj"
      },
      "source": [
        "Before anything else, you can execute the lines below. They load the relevant python libraries that will\n",
        "be useful in all the following. They only need to be executed once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VYyHetaIG1Hm"
      },
      "outputs": [],
      "source": [
        "# Let us import the relevant libraries here so we don't have to think about it\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import itertools as it\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WFjbNfhG1Ho"
      },
      "source": [
        "### Construction of the Markov (transition) matrix\n",
        "\n",
        "Start by constructing and filling the Markov matrix. Creating a\n",
        "matrix filled with zeros can be done with this code:\n",
        "```python\n",
        "n = 3\n",
        "dim = n**2\n",
        "markov = np.zeros([dim,dim])\n",
        "```\n",
        "Now add the code to fill the matrix. You can just fill in the elements\n",
        "by hand. For more challenge, you can try to fill the matrix for a generic\n",
        "lattice of size $n \\times n$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv5YKkr-zgTT"
      },
      "outputs": [],
      "source": [
        "#create Markov matrix\n",
        "n=3\n",
        "dim=n**2\n",
        "matrix=np.zeros([dim,dim])\n",
        "matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK60sqzCG1Hv"
      },
      "source": [
        "### Basic properties (Perron-Frobenius theorem) of the Markov matrix\n",
        "\n",
        "Check that the matrix satisfies the following criteria:\n",
        "\n",
        "   * The sum over the elements in a column is 1.\n",
        "   * There is at least one eigenvalue equal to 1.\n",
        "   * The corresponding eigenvector can be chosen to have only real positive elements.\n",
        "   \n",
        "Hint: You can find the eigenvalues and eigenvectors of a matrix with\n",
        "```python\n",
        "eigvals, eigvects = np.linalg.eig(markov)\n",
        "```\n",
        "The columns of `eigvects` are the eigenvectors. They are sorted in the\n",
        "same order as the eigenvalues in `eigvals`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1Qyyh4dzkLN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "rdvec=np.random.random([,9])\n",
        "np.dot(np.pow(T, 10), rdvec)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCbDdn27G1Hy"
      },
      "source": [
        "### Evolution of the probability distribution\n",
        "\n",
        "Let us now see how the probability distribution evolves. Start by\n",
        "generating a random initial normalized probability distribution.\n",
        "You can do this with this code:\n",
        "```python\n",
        "prob = np.random.rand(dim)\n",
        "prob /= np.sum(prob)\n",
        "```\n",
        "Now apply the Markov matrix to this probability distrubtion several times and see how\n",
        "it evolves. Acting with a matrix on a vector can be done simply with\n",
        "`np.dot(markov, prob)`. For the python afficionados, you can represent the vector of\n",
        "probabilities as a $3 \\times 3$ colormap for a better visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbVN5XEszmWO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5u7WPFfG1H0"
      },
      "source": [
        "### Non ergodic case\n",
        "\n",
        "The Markov matrix above is ergodic. Out of curiosity, let us see what happens if\n",
        "we are dealing with a non-ergodic matrix. To do that, let us investigate a Markov matrix\n",
        "with the same rules as above but with the difference that if a transition between\n",
        "the first row and the second row is found, one remains on the same cell. This\n",
        "effectively disconnects the first row from the others:\n",
        "\n",
        "   * Construct this new Markov matrix and check the normalization of the columns.\n",
        "   * What can you say about the eigenvalue spectrum in this case?\n",
        "   * Let a randomly chosen initial proability distribution evolve with the\n",
        "     Markov matrix.\n",
        "   * What happens in this case?\n",
        "   * Can you anticipate what will be the final stationary probability distribution\n",
        "     from the inital one?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UPAfR1-zodX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfA32iM5G1H3"
      },
      "source": [
        "## Markov matrix for a non-uniform distribution: Metropolis algorithm\n",
        "\n",
        "We now want to construct a Markov matrix that leads to a predefined non-uniform\n",
        "distribution of probabilities $\\pi(a)$ for the cells of the $3 \\times 3$ lattice. For concreteness,\n",
        "let us consider the special case where the probability for even cells is\n",
        "$1/13$ and that of odd cells is twice as large $2/13$. This probability is\n",
        "clearly normalized: $5 \\times (1/13) + 4 \\times (2/13) = 1$.\n",
        "\n",
        "In order to construct the matrix, we use the detailed balance condition\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi(a) \\, p_{a\\to b} = \\pi(b) \\, p_{b\\to a}\n",
        "\\end{equation}\n",
        "\n",
        "The Metropolis algorithm gives a recipe for $p_{a \\to b}$ that automatically\n",
        "satisfies the detailed balance above\n",
        "\n",
        "$$\n",
        "p_{a \\to b} = \\alpha \\times \\mathrm{min} \\big[ 1, \\frac{\\pi(b)}{\\pi(a)} \\big]\n",
        "\\qquad \\text{with} \\qquad\n",
        "\\sum_x p_{a \\to x} = 1\n",
        "$$\n",
        "\n",
        "This expression is valid for neighboring $a \\ne b$. Unlike the previous uniform\n",
        "case, the probability to remain in the same cell is obtained via the\n",
        "normalization, i.e.\n",
        "\n",
        "$$\n",
        "p_{a \\to a} = 1 - \\sum_{x \\ne a} p_{a \\to x}\n",
        "$$\n",
        "\n",
        "The parameter $\\alpha$ can be chosen at will as long as it still allows to\n",
        "define probabilities $p_{a \\to b} > 0$. You can check that this is the\n",
        "case as long as $0 < \\alpha \\le 1/4$. In the following, you can consider\n",
        "the case $\\alpha = 1/4$ which leads to a more rapid exploration of the\n",
        "cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SyUh0_pG1H4"
      },
      "source": [
        "### Construction of the Markov (transition) matrix\n",
        "\n",
        "Follow the same steps as above for this new non-uniform case:\n",
        "\n",
        "   * Construct the Markov matrix with the Metropolis recipe and check the normalization of the columns.\n",
        "   * Let a randomly chosen initial proability distribution evolve with the\n",
        "     Markov matrix.\n",
        "   * Do you obtain the expected result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g3d3pXhzsdE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBaQ15k7G1H8"
      },
      "source": [
        "### Eigenvalues of the Markov matrix\n",
        "\n",
        "If you were only given the Markov matrix, would there be a simple way to find the\n",
        "stationary distribution without having to apply the Markov matrix many times?\n",
        "Investigate the eigenvalue spectrum and the corresponding eigenvectors. What can you say?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJaAQaqXzu8K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1Lnlik7G1H9"
      },
      "source": [
        "### Convergence to the stationary distribution\n",
        "\n",
        "Above, we have show that the convergence to the stationary distribution\n",
        "is controlled by the modulus of the second largest eigenvalue $\\lambda_2$.\n",
        "Start from an initial random guess $\\pi^{t=0}$ and show how it converges\n",
        "to the stationary distribution $\\pi_1$ by plotting the distance\n",
        "$| \\pi^t - \\pi_1 |$ as a function of $t$. To observe the exponential\n",
        "convergence, you may want to use a semilog plot:\n",
        "```python\n",
        "plt.semilogy(dist, '-o')\n",
        "```\n",
        "where `dist` is an array. You can get the norm of a vector `v`\n",
        "with `np.linalg.norm(v)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbnPORk8zwZI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.7",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "a6b1c1e0b4cc2c7b94758dc48ca43f4e1f573c11345aa471dd73649661b4f98a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
