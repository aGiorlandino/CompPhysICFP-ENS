{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEQYvp4cfvS0"
      },
      "source": [
        "# Statistical averages and error bars\n",
        "\n",
        "When using stochastic methods such as Monte Carlo, it is very\n",
        "important to be able to give correct estimates of the statistical variance of computed physical observables.\n",
        "\n",
        "Let us suppose that a simulation yields the series of $N$ samples $\\xi_1, \\ldots, \\xi_N$. Every $\\xi_i$ in this series\n",
        "can be seen as a random variable whose average value is the physical observable $\\xi$ that we are trying to compute. The best estimate for $\\xi$ is **always** the sample average\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\xi \\leftarrow \\frac{1}{N} \\sum_{i=1}^N \\xi_i\n",
        "\\end{equation*}\n",
        "\n",
        "One has to be more careful when it comes to estimating the\n",
        "statistical error bar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1O3EX4FsnP6"
      },
      "source": [
        "## Independent samples\n",
        "\n",
        "The situation is easiest when the samples $\\xi_1, \\ldots, \\xi_N$ are all independent. This is for example the case when *direct sampling* is used. In this case, the statistical error for the\n",
        "mean value of $\\xi$ can be obtained from the central limit theorem\n",
        "and we have\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\mathrm{error} = \\frac{\\sigma}{\\sqrt{N}}\n",
        "\\end{equation*}\n",
        "\n",
        "where $\\sigma$ is the standard deviation of\n",
        "every single sample and can be estimated directly\n",
        "from the samples\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\sigma^2 = \\langle \\xi_k^2 \\rangle - \\langle \\xi_k \\rangle^2\n",
        "  \\leftarrow \\frac{1}{N} \\sum_{i=1}^N \\xi_i^2 -\n",
        "  \\left( \\frac{1}{N} \\sum_{i=1}^N \\xi_i \\right)^2\n",
        "\\end{equation*}\n",
        "\n",
        "From a practical point of view, if your samples are stored in an array `samples`, then the estimates for the mean and variance of the physical observable can be obtained with\n",
        "```python\n",
        "mean = np.average(samples)\n",
        "error = np.std(samples) / np.sqrt(samples.shape[0])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vQr9J2OyBxE"
      },
      "source": [
        "## Correlated samples\n",
        "\n",
        "Unfortunately, most of the time it is difficult to use direct sampling and a Markov chain Monte Carlo must be implemented. In that case, the samples $\\xi_1, \\ldots, \\xi_N$ are *not independent*, they are *correlated*. You can still compute the average of a physical observable from the sample average. But you have to be careful when estimating the error bar. There are 3 possibilities to find the error bar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPW1auBu5-WF"
      },
      "source": [
        "### Producing independent Markov chains\n",
        "\n",
        "The simplest solution is to generate several completely independent\n",
        "Markov chains (all of length $N$). The averages obtained for every chain are independent and we can use the formula above. What you will observe with correlated samples is that the error bar on the estimate from a given chain of length $N$ is larger than what it would be for a chain of independent samples\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\mathrm{error} = \\frac{\\sigma}{\\sqrt{N_\\mathrm{eff}}}\n",
        "  \\quad \\mathrm{with} \\quad N_\\mathrm{eff} = \\frac{N}{\\tau}\n",
        "\\end{equation*}\n",
        "\n",
        "where $N_\\mathrm{eff}$ can be seen as the number of effectively\n",
        "independent samples. The autocorrelation time $\\tau$ measures\n",
        "how many steps it takes to reach a new independent sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww4pw5Zw5-6S"
      },
      "source": [
        "### The bunching algorithm\n",
        "\n",
        "It is not very convenient to have to run a simulation several times in order to find the error bar. The bunching method (see Smac p.60-62)  is a way to compute the standard deviation from a single series of samples. The idea is to start from the original series $\\{\\xi^{(\\ell)}_i\\}$ with $N$ samples and then construct a new series $\\{ \\xi^{(\\ell+1)} \\}$ of $N/2$ samples by taking averages of pairs of consecutive samples of the original series:\n",
        "\n",
        "\\begin{equation*}\n",
        "    \\xi^{(\\ell+1)}_i = \\frac{1}{2} \\left( \\xi^{(\\ell)}_{2i-1} + \\xi^{(\\ell)}_{2i} \\right)\n",
        "\\end{equation*}\n",
        "  \n",
        "We can estimate the standard deviation of this new series with the naive formula above and then continue and bin the series again. If the bunching has been done many times, we can expect that the samples eventually become independant and that the standard deviation estimate will become correct. In practice, you can plot the standard deviation as a function of the bunching level $\\ell$ and see if it has a plateau where the estimate is faithful. Here is an example of a code that applies the bunching procedure starting\n",
        "from a list `samples` (the number of elements should be a power of 2)\n",
        "```python\n",
        "n_levels = 16\n",
        "error = np.zeros(n_levels)\n",
        "for i in range(n_levels):\n",
        "    error[i] = np.std(samples) / np.sqrt(samples.shape[0])\n",
        "    samples = np.average(samples.reshape(-1, 2), axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [2, 3],\n",
              "       [4, 5],\n",
              "       [6, 7]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "x=np.arange(8)\n",
        "x=x.reshape(-1,2)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Tgk4Xa5_Oy"
      },
      "source": [
        "### Using the autocorrelation function\n",
        "\n",
        "The degree of correlations between samples can be analyzed using the covariance matrix\n",
        "\n",
        "\\begin{equation*}\n",
        "  C_{ij} = \\left \\langle (\\xi_i -\\langle \\xi_i \\rangle) \\, (\\xi_j -\\langle \\xi_j \\rangle) \\right \\rangle\n",
        "\\end{equation*}\n",
        "\n",
        "After the transient, the covariance is just a function $C(n)$ of the\n",
        "distance $n = |i-j|$. Clearly, $C(n)$ is vanishing if the samples are independent. When they are correlated $C(n)$ typically decays exponentially with $n$\n",
        "\n",
        "\\begin{equation*}\n",
        "  C(n) \\simeq \\sigma^2 \\exp(-2 n / \\tau)\n",
        "\\end{equation*}\n",
        "\n",
        "Again, the autocorrelation time $\\tau$ appears as it controls\n",
        "how quickly samples become independent. It can be obtained from\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\tau = \\frac{1}{\\sigma^2} \\left( C(0) + 2 \\sum_{n=1}^N C(n) \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "The error is then simply\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\mathrm{error} = \\frac{\\sigma \\sqrt{\\tau}}{\\sqrt{N}}\n",
        "  = \\sqrt{\\frac{C(0) + 2 \\sum_{n=1}^N C(n)}{N}}\n",
        "\\end{equation*}"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
